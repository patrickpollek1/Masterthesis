# Interpreting-Neural-Networks-Under-Latent-Confounding

## Description

This repository contains the culmination of my master's thesis conducted at the University of Vienna. The core focus of my research was to address the challenges of interpretability in machine learning models that are utilized for high-risk decision-making processes. Despite their predictive accuracy, these models often lack transparency due to their complexity, leading to ethical concerns and obstacles to human understanding.

One of the key issues explored in this thesis is the role of confounding variablesâ€”often latent factors that influence both the predictors and the outcomes within a model, thereby affecting the reliability of interpretations. This research is particularly concerned with the effect of such variables on the interpretability of neural networks.

Through this investigative effort, the thesis provides an in-depth analysis of how confounding variables impact the interpretation of neural networks. By employing techniques such as Individual Conditional Expectation (ICE) plots and SHAP value visualizations, the study examines the sensitivities of simple feed-forward neural networks to hidden confounders, with a specific focus on feature importance.

To address these complexities, the thesis introduces a novel approach utilizing a model based on a variational autoencoder (VAE). This model demonstrates that it can produce more consistent and intuitive interpretations in comparison to traditional models. This assertion is backed by experimental validation across three distinct datasets, which collectively suggest that confounding variables significantly affect the perceived importance of features, especially those that exhibit high correlation.

The repository hosts the models, datasets, interpretability analyses, and the variational autoencoder code, all of which contribute to a better understanding of the interpretive challenges posed by confounding variables in neural networks.


## Table of Contents
- [Code Structure](#code-structure)
- [Models](#models)
- [Datasets](#datasets)
- [Interpretability](#interpretability)
- [Variational Autoencoder (VAE)](#variational-autoencoder-vae)
- [Contact](#contact)

## Code Structure
## Models

## Datasets

## Interpretability

## Variational Autoencoder (VAE)

## Contact

How to reach you for more information or queries about the project.

- Patrick POLLEK
- ppollek1234@gmail.com
- https://www.linkedin.com/in/patrick-pollek-24104a152/
